





from scipy import stats
import pandas as pd
import numpy as np


# 데이터
scores = [75, 80, 68, 72, 77, 82, 81, 79, 70, 74, 76, 78, 81, 73, 81, 78, 75, 72, 74, 79, 78, 79]


mu= 75
sample_mean = np.mean(scores)
sample_mean


stats.ttest_1samp(scores,mu, alternative = 'greater')


# pvalue <0.05 귀무가설 탈락, 모평균은 75보다 크다고 할 수 있다.


# 검정통계량 
# t 통계량은 **“표본평균과 모평균이 얼마나 떨어져 있는지를 표준오차 단위로 나타낸 값”**입니다.


# help(ttest_1samp)


# import scipy.stats
# dir(scipy.stats)














from scipy import stats

# 가설 설정
# H0 : 약물을 복용한 그룹과 복용하지 않은 그룹의 평균 체온은 유의미한 차이가 없다.
# H1 : 약물을 복용한 그룹과 복용하지 않은 그룹의 평균 체온은 유의미한 차이가 있다. 


# 데이터 수집
group1 = [36.8, 36.7, 37.1, 36.9, 37.2, 36.8, 36.9, 37.1, 36.7, 37.1]
group2 = [36.5, 36.6, 36.3, 36.6, 36.9, 36.7, 36.7, 36.8, 36.5, 36.7]


stats.ttest_ind(group1,group2) # two-sided


stats.levene(group1,group2) # 등분산 가정을 하지않고 직접 검증 >>등분산 


stats.ttest_ind(group1,group2,equal_var=True) # two-sided  equal_var=True 명시. 기본값.





# help(stats.ttest_ind)








import pandas as pd
df=pd.read_csv('data/high_blood_pressure.csv')


df.head(2)


stats.ttest_rel(df['bp_post'],df['bp_pre'],alternative = 'less')











# 각 그룹의 데이터
groupA = [85, 92, 78, 88, 83, 90, 76, 84, 92, 87]
groupB = [79, 69, 84, 78, 79, 83, 79, 81, 86, 88]
groupC = [75, 68, 74, 65, 77, 72, 70, 73, 78, 75]





# dir(scipy.stats)


from scipy.stats import f_oneway


# help(f_oneway)


stats,p_val= f_oneway(groupA,groupB,groupC)


round(stats,2)


round(p_val,6)





import pandas as pd

groupA = [85, 92, 78, 88, 83, 90, 76, 84, 92, 87]
groupB = [79, 69, 84, 78, 79, 83, 79, 81, 86, 88]
groupC = [75, 68, 74, 65, 77, 72, 70, 73, 78, 75]

data = {'GroupA': groupA, 'GroupB': groupB, 'GroupC': groupC}


df_wide = pd.DataFrame(data)


df_wide


df_long = df_wide.melt(value_vars=['GroupA', 'GroupB', 'GroupC'], var_name='Group', value_name='Score')


df_long


# help(df.melt)


df_pivot = df_long.reset_index().pivot(index='index', columns='Group', values='Score')


df_pivot








# dir(scipy.stats)
from scipy.stats import shapiro
# help(shapiro)



data = [75, 83, 81, 92, 68, 77, 78, 80, 85, 95, 79, 89]


statistic, p_val = shapiro(data)
statistic, p_val


# 귀무가설 기각 실패. 정규 분포를 따른다. 










import pandas as pd
from sklearn.datasets import load_iris

# iris 데이터셋 로드
iris = load_iris()
df = pd.DataFrame(iris.data, columns=iris.feature_names)


cor=df.corr()


df.head()


cor.head()


round(cor.iloc[0,1],2)








import pandas as pd
df= pd.read_csv('data/Titanic.csv')


from statsmodels.formula.api import logit 


df.info()


for col in ['Pclass','Gender','SibSp','Parch','Survived']:
    print(df[col].value_counts())


formula = 'Survived ~ C(Pclass) + Gender + SibSp + Parch'

model = logit(formula, df).fit()
model.summary()
model.params


logP= model.params['Parch']
logP


np.exp(logP)
# Parch가 1 증가할 때, 생존할 **오즈(odds)**는 약 0.95배로 감소합니다.
# 즉, Parch가 많을수록 생존 확률이 약간 낮아지는 경향이 있음 (다른 변수들이 동일할 때)
# 생존확률 p = odds / (1+odds)


# C()는 독립변수가 숫자일 때, 범주형으로 처리하라는 명령어
# 종속변수는 모델 종류에 따라 자동으로 적절히 처리됨


























import pandas as pd
df = pd.DataFrame({
    'A':[77, 75, 82, 80, 81, 83, 84, 76, 75, 87],
    'B':[80, 74, 77, 79, 71, 74, 78, 69, 70, 72],
})


from scipy import stats


t_stats, p_val = stats.ttest_ind(df['A'],df['B']) # 기본 two-sided 사용
t_anova, p_anova = stats.f_oneway(df['A'],df['B'])


print(f"독립표본 t검정 : {t_stats}, {p_val}")


# 


print(f"ANOVA t검정 : {t_anova}, {p_anova}")











# 관측된 빈도
observed_frequencies = [30, 60, 50, 40, 20]
# 기대된 빈도 
expected_frequencies = [200 * 0.20, 200 * 0.30, 200 * 0.25, 200 * 0.15, 200 * 0.10]


from scipy.stats import chisquare


chisquare(observed_frequencies,expected_frequencies)
# chisquare(f_obs=observed_frequencies, f_exp=expected_frequencies)


# 귀무가설 : 올해 졸업생들의 전공 선택 분포는 과거와 동일하다
# 대립가설 : 올해 졸업생들의 전공 선택 분포는 과거와 다르다


# ✅ 보너스: 자유도 (df)

# 자유도는:
# df=k−1=5−1=4
# df=k−1=5−1=4

# → 이 정보는 시험에서 검정표나 임계값으로 기각여부를 따질 때 참고할 수 있어요.


























import pandas as pd
# 데이터
df = pd.DataFrame({
    'transaction': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    '빼빼로': [1, 0, 1, 1, 0, 1, 1, 0, 1, 1],
    '딴짓초코': [0, 1, 1, 0, 1, 0, 1, 1, 0, 0],
    '양조위빵': [1, 0, 0, 1, 1, 1, 0, 0, 1, 0],
    '오징어칩': [0, 1, 1, 0, 0, 1, 0, 1, 1, 1],
    '초코파이': [1, 1, 0, 0, 1, 0, 1, 1, 0, 0]
})
df


# 각 문제의 계산을 위한 데이터 준비
total = df.shape[0]
pepero = df['빼빼로'].sum()


# 문제 1: 빼빼로와 딴짓초코가 함께 팔린 거래의 지지도
pepero_and_choco = len(df[(df['빼빼로'] == 1) & (df['딴짓초코'] == 1)])
print(pepero_and_choco / total)


# 문제 2: 빼빼로가 팔린 거래 중 빼빼로와 오징어칩이 함께 팔린 거래의 신뢰도
pepero_and_squid = len(df[(df['빼빼로'] == 1) & (df['오징어칩'] == 1)])
print(pepero_and_squid / pepero)


# 문제 3: 빼빼로와 양조위빵의 향상도
pepero_and_bread = len(df[(df['빼빼로'] == 1) & (df['양조위빵'] == 1)])
bread = df['양조위빵'].sum()
print((pepero_and_bread / pepero) / (bread / total))


bread


pepero_and_bread











from scipy.stats import poisson


# help(poisson) 
#     pmf(k, mu, loc=0)
#         Probability mass function.
#     cdf(k, mu, loc=0)
#         Cumulative distribution function.



from scipy.stats import poisson

# 평균 발생 횟수 (하루에 잡지를 구매하는 고객 수)
lambda_ = 3


# 하루에 정확히 5명의 고객이 잡지를 구매할 확률
print(poisson.pmf(5, lambda_))


# 10%


# 하루에 적어도 2명의 고객이 잡지를 구매할 확률
print(1 - poisson.cdf(1, lambda_))











male = [100,200]
female= [130,170]

data= {"남자": male, "여자":female}
df=pd.DataFrame(data, index= ['합격','불합격'])


df


statistic, pvalue, dof,expected= stats.chi2_contingency(df)


print (statistic, pvalue, dof,expected)


# 결과 출력
print(f'검정통계량: {statistic}')
print(f'p-value: {pvalue}')
print(f'남자의 합격 기대빈도: {expected[0][0]}')


stats.chi2_contingency(df)


# 유의수준 5%에서 성별과 시험 합격 간에는 통계적으로 유의한 관계가 있으며, 두 변수는 독립적이지 않다.
# 특히, 남자는 기대보다 적게 합격했고, 여자는 기대보다 많이 합격하였다.
# [[115. 115.], [185. 185.]]는 chi2_contingency() 함수의 반환값 중 **기대빈도표 (expected)**이고,그 안에 있는 185는 불합격자의 기대값








df= pd.read_csv('data/type3/t3_success.csv')


df


from scipy.stats import binom


# 베르누이 분포 : 각 시도의 성공 확률 계산
total_attempts = len(df)
number_of_successes = df['Success'].sum()
success_probability = number_of_successes / total_attempts


success_probability


# 이항분포 : 100 번의 시도 중 정확히 60번 성공할 확률 계산
n=100 # 시도 횟수
k=60 # 성공 횟수
prob_60_success= binom.pmf(k,n,success_probability)


print(prob_60_success)


# help(binom) 
# pmf(k, n, p, loc=0)
#         Probability mass function.
# cdf(k, n, p, loc=0)
#         Cumulative distribution function.



# 60번 이상 성공할 확률 계산 
# over_60= 1-binom.cdf(k,n,success_probability)
over_60 = 1 - binom.cdf(59, n=100, p=0.6)


print(over_60)


help(binom.cdf)





from scipy.stats import binom

prob = 1 - binom.cdf(7, n=10, p=0.7)
print(f"P(X ≥ 8) = {round(prob, 4)}")


# 정확히 60번 성공할 확률은?
binom.pmf(60, n=100, p=0.6)














df=pd.read_csv('data/type3/daily_temperatures.csv')


df.head()


temperature_data=df.copy()


import pandas as pd
from scipy import stats
# CSV 파일 불러오기
# 점추정: 샘플 평균 계산
sample_mean = temperature_data['Daily Average Temperature'].mean()

# 구간추정: 샘플 표준편차 계산 및 신뢰구간 계산
confidence_level = 0.95
sample_std = temperature_data['Daily Average Temperature'].std()  # 자유도 n-1로 설정 판다스는 자동으로 표본표준편차를 계산. 넘파이는 설정해야함.  np.std(df,ddof=1) 
n_samples = len(temperature_data)

# 95% 신뢰구간 계산
confidence_interval = stats.t.interval(confidence_level, df=n_samples-1, loc=sample_mean, scale=sample_std/(n_samples**0.5))

sample_mean, confidence_interval





temp= df['Daily Average Temperature'].rename("온도")


temp


df = df.rename(columns={'Daily Average Temperature': '온도'})


std = df['온도'].std(ddof=1) # ddof=1 이 기본값이긴함
std


import numpy as np
import scipy.stats as stats

# 예시: df['온도']에 평균 온도 데이터가 있다고 가정
mean = df['온도'].mean()                     # 점추정
std = df['온도'].std(ddof=1)                # 표본 표준편차
n = len(df['온도'])                          # 표본 크기
se = std / np.sqrt(n)                        # 표준오차

# 95% 신뢰구간 (t 분포 사용)
t_crit = stats.t.ppf(0.975, df=n-1)          # 양쪽 2.5%씩 잘림

ci_lower = mean - t_crit * se
ci_upper = mean + t_crit * se

# 출력
print(f"점추정 (표본 평균): {round(mean, 2)}")
print(f"95% 신뢰구간: ({round(ci_lower, 2)}, {round(ci_upper, 2)})")






# help(stats.t.interval)


import numpy as np
import scipy.stats as stats

# 평균, 표준편차, 표본 크기
mean = df['온도'].mean()
std = df['온도'].std(ddof=1)
n = len(df['온도'])
se = std / np.sqrt(n)

# 95% 신뢰구간 계산 (interval 함수 사용)
ci_lower, ci_upper = stats.t.interval(
    confidence=0.95,         # 신뢰수준
    df=n - 1,           # 자유도
    loc=mean,           # 중심 (표본 평균)
    scale=se            # 표준오차
)

# 출력
print(f"점추정 (표본 평균): {round(mean, 2)}")
print(f"95% 신뢰구간: ({round(ci_lower, 2)}, {round(ci_upper, 2)})")



help(stats.t.cdf)




















df=pd.read_csv('data/type3/christmas_decoration_sales.csv')


df.head(2)


# ols 다중선형회귀 분석 후 anova table 로 이원 분산 분석을 확인
from statsmodels.formula.api import ols
import statsmodels.api





df.info()


# 종속변수는 판매량, 독립변수는 장식 종류 + 지역 + 상호작용











model.summary()


# dir(statsmodels.formula.api)


# dir(statsmodels.api.stats.anova_lm)





df=pd.read_csv('data/type3/Customer_Data.csv')


df.head(2)


df.info()


train= df.iloc[:350]
test=df.iloc[350:]


train.shape,test.shape


import statsmodels.api as sm
import numpy as np












import pandas as pd
from statsmodels.formula.api import logit


df= pd.read_csv('data/type3/Customer_Data.csv')


train = df.iloc[:300]
test = df.iloc[300:]


model = logit('purchase ~ age + income + marital_status', data=train).fit()


print(model.summary())


model.params





# 잔차이탈도	-2 * 로그 우도 (Log-Likelihood)	모델의 적합도 지표
print(round(-2 * model.llf,2))


# summary 만 보고 계산하면 소수점이 모두 나오지 않기 때문에 차이가 있음
-2 * -206.22








data = pd.read_csv('data/type3/t3_regression_data.csv')


data.head()


data.info()


# 필요한 라이브러리 임포트
import statsmodels.formula.api as smf

# 1. 모든 변수를 사용하여 OLS 모델 적합
model_full = smf.ols('y ~ x1 + x2 + x3 + x4', data=data).fit()
print("1. 전체 모델:")
print(model_full.summary())


# 가장 무의미한 변수 x4 (p_value : 0.796)


# 2. 유의미하지 않은 변수를 제거한 후 모델 재적합
# 유의미한 변수만 골라 새로운 모델 적합
model_refit = smf.ols('y ~ x1 + x2 + x3', data=data).fit()
print("\n2. 유의미한 변수로 재적합한 모델:")
print(model_refit.summary())


# 3. 초기 모델의 R-squared 값 계산
print("\n3. 초기 모델의 R-squared 값:", model_refit.rsquared)


# 4. 새로운 데이터(x1=5, x2=12, x3=10, x4=3)에 대해 y 예측
new_data = pd.DataFrame({'x1': [5], 'x2': [12], 'x3': [10], 'x4': [3]})
y_pred = model_full.predict(new_data)
print("\n4. 새로운 데이터 포인트에 대한 예측값:", y_pred.iloc[0])


# 5. 독립 변수 간의 상관관계 계산
correlation_matrix = data.corr()
print("\n5. 예측 변수 간의 상관관계 행렬:")
print(correlation_matrix)


# 6. x1과 x2만을 사용한 모델 적합 및 R-squared 값
model_x1_x2 = smf.ols('y ~ x1 + x2', data=data).fit()
print("\n6. x1과 x2만 사용한 모델의 R-squared 값:", model_x1_x2.rsquared)


# 7. 잔차 분석 수행
residuals = model_full.resid
print("\n7. 잔차의 표준편차:")
print(residuals.std())


# 8. 1번 모델에서 새로운 데이터에 대해 y의 신뢰구간 하한(97% 신뢰수준)을 구하세요.
pred_conf_int_97 = model_full.get_prediction(new_data).summary_frame(alpha=0.03)
conf_lower_97 = pred_conf_int_97['mean_ci_lower'].iloc[0]
print("10. y의 신뢰구간 하한(97% 신뢰수준):", conf_lower_97)


# pred = model.get_prediction(new_data).summary_frame(alpha=0.05)
# print(pred[['mean', 'mean_ci_lower', 'mean_ci_upper']])





# 9. 1번 모델에서 새로운 데이터에 대해 y의 예측구간 상한(97% 신뢰수준)을 구하세요.
pred_upper_97 = pred_conf_int_97['obs_ci_upper'].iloc[0]
print("11. y의 예측구간 상한(97% 신뢰수준):", pred_upper_97)


pred_conf_int_97








import pandas as pd
data = {
    "weight": [630, 610, 625, 615, 622, 618, 623, 619, 620, 624, 616, 621, 617, 629, 626, 620, 618, 622, 625, 615, 
               628, 617, 624, 619, 621, 623, 620, 622, 618, 625, 616, 629, 620, 624, 617, 621, 623, 619, 625, 618,
               622, 620, 624, 617, 621, 623, 619, 625, 618, 622]
}
df = pd.DataFrame(data)





# 설정값
med = 620


df.median()


from scipy.stats import wilcoxon

wilcoxon(df['weight']- med,alternative='greater')


# help(wilcoxon)






